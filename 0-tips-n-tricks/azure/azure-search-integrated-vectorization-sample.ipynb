{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search integrated vectorization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AZURE_AI_SEARCH_ENDPOINT = os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"]\n",
    "AZURE_AI_SEARCH_API_KEY = os.environ[\"AZURE_AI_SEARCH_API_KEY\"]\n",
    "AZURE_STORAGE_ACC_CONNECTION_STRING = os.environ[\"AZURE_STORAGE_ACC_CONNECTION_STRING\"]\n",
    "BLOB_CONTAINER_NAME = os.environ[\"BLOB_CONTAINER_NAME\"]\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_VERSION = os.environ[\"AZURE_OPENAI_VERSION\"]\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"]\n",
    "\n",
    "DOC_INTELLIGENCE_ENDPOINT = os.environ[\"DOC_INTELLIGENCE_ENDPOINT\"]\n",
    "DOC_INTELLIGENCE_KEY = os.environ[\"DOC_INTELLIGENCE_KEY\"]\n",
    "\n",
    "SEARCH_TARGET_INDEX_NAME = os.getenv(\"AZURE_AI_FISHING_INDEX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Blob Storage and load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fishingguide1.pdf\n",
      "fishingguide2.pdf\n",
      "fishingguide3.pdf\n",
      "fishingguide4.pdf\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient  \n",
    "import os\n",
    "\n",
    "# connect to blob storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_ACC_CONNECTION_STRING)\n",
    "container_client = blob_service_client.get_container_client(BLOB_CONTAINER_NAME)\n",
    "\n",
    "# create if doesn't exist\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "for blob in container_client.list_blob_names():\n",
    "    print(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a blob data source connector on Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'data-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection\n",
    ")\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(AZURE_AI_SEARCH_ENDPOINT, AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "container = SearchIndexerDataContainer(name=BLOB_CONTAINER_NAME)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{BLOB_CONTAINER_NAME}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=AZURE_STORAGE_ACC_CONNECTION_STRING,\n",
    "    container=container\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX SUCCESSFULLY CREATED: fishing-index-d\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIParameters,\n",
    "    SemanticConfiguration,\n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "# Create a search index  \n",
    "index_client = SearchIndexClient(endpoint=AZURE_AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))  \n",
    "fields = [  \n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),  \n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"keyphrases\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True, sortable=False, facetable=False),\n",
    "    SearchField(name=\"url\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),  \n",
    "]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(  \n",
    "            name=\"myHnsw\",  \n",
    "            parameters=HnswParameters(  \n",
    "                m=4,  \n",
    "                ef_construction=400,  \n",
    "                ef_search=500,  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "        ExhaustiveKnnAlgorithmConfiguration(  \n",
    "            name=\"myExhaustiveKnn\",  \n",
    "            parameters=ExhaustiveKnnParameters(  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myExhaustiveKnnProfile\",  \n",
    "            algorithm_configuration_name=\"myExhaustiveKnn\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                resource_uri=AZURE_OPENAI_ENDPOINT,  \n",
    "                deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,  \n",
    "                api_key=AZURE_OPENAI_API_KEY,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "    prioritized_fields=SemanticPrioritizedFields(  \n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "# Create the semantic search with the configuration  \n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])  \n",
    "  \n",
    "# Create the search index\n",
    "index = SearchIndex(name=SEARCH_TARGET_INDEX_NAME, fields=fields, vector_search=vector_search, semantic_search=semantic_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"INDEX SUCCESSFULLY CREATED: {result.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKILLSET SUCCESSFULLY CREATED: fishing-index-d-skillset\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    KeyPhraseExtractionSkill,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = f\"{SEARCH_TARGET_INDEX_NAME}-skillset\"  \n",
    "  \n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=1000,  \n",
    "    page_overlap_length=300,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_uri=AZURE_OPENAI_ENDPOINT,  \n",
    "    deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,  \n",
    "    api_key=AZURE_OPENAI_API_KEY,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "    ],  \n",
    ")\n",
    "\n",
    "key_phrases_skill = KeyPhraseExtractionSkill(\n",
    "    description=\"Extract key phrases from document pages\",\n",
    "    context=\"/document/pages/*\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"keyPhrases\", target_name=\"keyphrases\")\n",
    "    ],\n",
    ")\n",
    "  \n",
    "index_projections = SearchIndexerIndexProjections(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=SEARCH_TARGET_INDEX_NAME,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
    "                InputFieldMappingEntry(name=\"keyphrases\", source=\"/document/pages/*/keyphrases\"),\n",
    "                InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\"),  \n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),\n",
    "                InputFieldMappingEntry(name=\"url\", source=\"/document/metadata_storage_path\"),\n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=[split_skill, key_phrases_skill, embedding_skill],  \n",
    "    index_projections=index_projections,  \n",
    ")  \n",
    "  \n",
    "client = SearchIndexerClient(endpoint=AZURE_AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"SKILLSET SUCCESSFULLY CREATED: {skillset.name}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fishing-index-d-indexer is created and running. If queries return no results, please wait a bit and try again.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    FieldMapping\n",
    ")\n",
    "\n",
    "# create an indexer  \n",
    "indexer_name = f\"{SEARCH_TARGET_INDEX_NAME}-indexer\"  \n",
    "  \n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=SEARCH_TARGET_INDEX_NAME,  \n",
    "    data_source_name=data_source.name,\n",
    "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
    "    field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\"),\n",
    "                    FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"url\")]\n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(endpoint=AZURE_AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer\n",
    "indexer_client.run_indexer(indexer_name)  \n",
    "print(f' {indexer_name} is created and running. If queries return no results, please wait a bit and try again.')  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_id: aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGU0LnBkZg2\n",
      "chunk_id: 52aee84185bf_aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGU0LnBkZg2_pages_33\n",
      "Score: 0.7837893\n",
      "Content: government has also established criteria for environmental testing of pharmaceuticals to \n",
      "improve the environmental health near fish farms. These measures are aimed at \n",
      "minimizing harmful effects of aquaculture on the environment and indigenous wild fish \n",
      "populations\n",
      "\n",
      "FISHERY SECTOR DEVELOPMENT\n",
      "\n",
      "Constraints\n",
      "\n",
      "Given that Norway exports a significant proportion of its fish, global market conditions are \n",
      "particularly important. World market prices for frozen finfish fillets have been falling \n",
      "because of increased availability of cheaper products from Russia, Alaska and South \n",
      "America. European Union trade barriers have also have increased in the form of customs \n",
      "duty on processed fish products. It reduced profitability of the Norwegian fisheries sector \n",
      "\n",
      "\n",
      "\n",
      "and will continue to be important determinants in the sectorâ€™s performance.\n",
      "\n",
      "Development strategies and plans\n",
      "\n",
      "A master plan for Norwegian fisheries development was adopted by the Norwegian \n",
      "Parliament (Storting) in 1998.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"Which is more comprehensive, Northwind Health Plus vs Northwind Standard?\"  \n",
    "  \n",
    "search_client = SearchClient(AZURE_AI_SEARCH_ENDPOINT, SEARCH_TARGET_INDEX_NAME, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "query = \"What is the population of Norway?\"  \n",
    "  \n",
    "search_client = SearchClient(AZURE_AI_SEARCH_ENDPOINT, SEARCH_TARGET_INDEX_NAME, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    top=2\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
