{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search integrated vectorization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AZURE_AI_SEARCH_ENDPOINT = os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"]\n",
    "AZURE_AI_SEARCH_API_KEY = os.environ[\"AZURE_AI_SEARCH_API_KEY\"]\n",
    "AZURE_STORAGE_ACC_CONNECTION_STRING = os.environ[\"AZURE_STORAGE_ACC_CONNECTION_STRING\"]\n",
    "BLOB_CONTAINER_NAME = os.environ[\"BLOB_CONTAINER_NAME\"]\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_VERSION = os.environ[\"AZURE_OPENAI_VERSION\"]\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"]\n",
    "\n",
    "DOC_INTELLIGENCE_ENDPOINT = os.environ[\"DOC_INTELLIGENCE_ENDPOINT\"]\n",
    "DOC_INTELLIGENCE_KEY = os.environ[\"DOC_INTELLIGENCE_KEY\"]\n",
    "\n",
    "SEARCH_TARGET_INDEX_NAME = os.getenv(\"AZURE_AI_FISHING_INDEX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Blob Storage and load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fishingguide1.pdf\n",
      "fishingguide2.pdf\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient  \n",
    "import os\n",
    "\n",
    "# connect to blob storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_ACC_CONNECTION_STRING)\n",
    "container_client = blob_service_client.get_container_client(BLOB_CONTAINER_NAME)\n",
    "\n",
    "# create if doesn't exist\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "for blob in container_client.list_blob_names():\n",
    "    print(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a blob data source connector on Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'data-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection\n",
    ")\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(AZURE_AI_SEARCH_ENDPOINT, AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "container = SearchIndexerDataContainer(name=BLOB_CONTAINER_NAME)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{BLOB_CONTAINER_NAME}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=AZURE_STORAGE_ACC_CONNECTION_STRING,\n",
    "    container=container\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fishing-index-d created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIParameters,\n",
    "    SemanticConfiguration,\n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "# Create a search index  \n",
    "index_client = SearchIndexClient(endpoint=AZURE_AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))  \n",
    "fields = [  \n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),  \n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),  \n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),  \n",
    "]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(  \n",
    "            name=\"myHnsw\",  \n",
    "            parameters=HnswParameters(  \n",
    "                m=4,  \n",
    "                ef_construction=400,  \n",
    "                ef_search=500,  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "        ExhaustiveKnnAlgorithmConfiguration(  \n",
    "            name=\"myExhaustiveKnn\",  \n",
    "            parameters=ExhaustiveKnnParameters(  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myExhaustiveKnnProfile\",  \n",
    "            algorithm_configuration_name=\"myExhaustiveKnn\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                resource_uri=AZURE_OPENAI_ENDPOINT,  \n",
    "                deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,  \n",
    "                api_key=AZURE_OPENAI_API_KEY,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "    prioritized_fields=SemanticPrioritizedFields(  \n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "# Create the semantic search with the configuration  \n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])  \n",
    "  \n",
    "# Create the search index\n",
    "index = SearchIndex(name=SEARCH_TARGET_INDEX_NAME, fields=fields, vector_search=vector_search, semantic_search=semantic_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"INDEX SUCCESSFULLY CREATED: {result.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKILLSET SUCCESSFULLY CREATED: fishing-index-d-skillset\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = f\"{SEARCH_TARGET_INDEX_NAME}-skillset\"  \n",
    "  \n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=2000,  \n",
    "    page_overlap_length=500,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_uri=AZURE_OPENAI_ENDPOINT,  \n",
    "    deployment_id=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,  \n",
    "    api_key=AZURE_OPENAI_API_KEY,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "index_projections = SearchIndexerIndexProjections(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=SEARCH_TARGET_INDEX_NAME,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),  \n",
    "                InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\"),  \n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=[split_skill, embedding_skill],  \n",
    "    index_projections=index_projections,  \n",
    ")  \n",
    "  \n",
    "client = SearchIndexerClient(endpoint=AZURE_AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"SKILLSET SUCCESSFULLY CREATED: {skillset.name}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fishing-index-d-indexer is created and running. If queries return no results, please wait a bit and try again.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    FieldMapping\n",
    ")\n",
    "\n",
    "# Create an indexer  \n",
    "indexer_name = f\"{SEARCH_TARGET_INDEX_NAME}-indexer\"  \n",
    "  \n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=SEARCH_TARGET_INDEX_NAME,  \n",
    "    data_source_name=data_source.name,  \n",
    "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
    "    field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")]  \n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(endpoint=AZURE_AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer\n",
    "indexer_client.run_indexer(indexer_name)  \n",
    "print(f' {indexer_name} is created and running. If queries return no results, please wait a bit and try again.')  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_id: aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGUyLnBkZg2\n",
      "chunk_id: c8d41191c3e1_aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGUyLnBkZg2_pages_2\n",
      "Score: 0.7718277\n",
      "Content: Regulations ...........................................................................  42\n",
      "\n",
      "Invasive Species .................................................................................  45\n",
      "\n",
      "More Information ...............................................................................  46\n",
      "\n",
      "Map of State Parks That Have Fishing.........................................  48\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "Learn to Fish\n",
      "BASIC FISHING TACKLE\n",
      "\n",
      "Hooks \n",
      "\n",
      "Hooks come in an assortment of sizes and styles. Circle hooks are \n",
      "great for beginners and safe for fish. If you plan to release your catch, \n",
      "use barbless hooks or bend down the barb to make it easier to remove \n",
      "the hook. Choose the size of hook for the species of fish you are trying \n",
      "to catch and the type of bait you are using. Ask a seasoned angler or a \n",
      "bait and tackle dealer for suggestions.\n",
      "\n",
      "Line\n",
      "\n",
      "Fishing line comes in pound-test (the line size or strength). The larger \n",
      "the line size the stronger it is. Six-pound test line is more flexible but is \n",
      "not as strong as 12-pound test line. Match your fishing line to your rod \n",
      "and reel capability and the species of fish you want to catch. Using \n",
      "heavier line or higher pound test than needed may reduce the number \n",
      "of hits or strikes you get because heavier line is more visible to fish.\n",
      "\n",
      "Sinkers\n",
      "\n",
      "Sinkers are weights used to cast your bait, take bait to the bottom, \n",
      "hold bait in place, or keep your bobber upright. Use lead-free sinkers \n",
      "when possible. Sinkers are designed in several different shapes and \n",
      "sizes for various types of fishing techniques. They range in size from \n",
      "small (a fraction of an ounce) BB split shot to large five-pound weights \n",
      "used in offshore fishing. For a basic tackle rig, place your sinker \n",
      "approximately four inches above the hook to allow live bait to look \n",
      "natural to the fish.\n",
      " \n",
      "\n",
      "Bobbers\n",
      "\n",
      "Bobbers are used for three reasons. They keep your bait where the \n",
      "fish are biting, they keep bait off the bottom, and they let you know \n",
      "when you’re getting a bite.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"Which is more comprehensive, Northwind Health Plus vs Northwind Standard?\"  \n",
    "  \n",
    "search_client = SearchClient(AZURE_AI_SEARCH_ENDPOINT, SEARCH_TARGET_INDEX_NAME, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_id: aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGU0LnBkZg2\n",
      "chunk_id: b748f0d7f7e0_aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGU0LnBkZg2_pages_12\n",
      "Score: 0.03055555745959282\n",
      "Content: vessels and processing plants are largely privately owned and run, with most \n",
      "services and infrastructure being public.\n",
      "\n",
      "Supply and demand\n",
      "\n",
      "Norwegians have an annual supply of 54.7 kg of fish and fish products per caput. Over the \n",
      "last decade there has been an increase in consumption by 30 to 50 year olds, while other \n",
      "age groups have started to eat less fish and fish any products. There is a long tradition of \n",
      "fish consumption in Norway, with about 30 per cent originating from household \n",
      "recreational fishing in the coastal zone.\n",
      "\n",
      "Fish accounts for approximately 16 per cent of the average daily protein intake of \n",
      "Norwegians (17.1 g per day per capita). This is less than provided by meat (20.7 g per day \n",
      "per capita), milk (23.1 g per day per capita) and cereal (excluding beer, 29.9 g per day per \n",
      "capita) protein sources. The supply of fish consumed mainly comes from freshwater fish \n",
      "and demersal marine species, accounting for approximately 19 kg per capita per year each.\n",
      "\n",
      "There is an increasing national and international demand for certification that the seafood \n",
      "is safe to eat and of good quality. In response to this the Norwegian Food Safety Authority \n",
      "was launched at the beginning of 2004; it is responsible for seafood safety and quality, as \n",
      "well as fish health and ethically acceptable farming of fish.\n",
      "\n",
      "Trade \n",
      " \n",
      "Imports\n",
      "\n",
      "In 2003 the value of imports was US$ 560.5 million, which is approximately 45 per cent \n",
      "more than in 1995. The volume also increased over this period of time. The largest single \n",
      "import species was fresh mackerel, destined for reduction and representing approximately \n",
      "19 per cent of the imports. Since the mid-1990s both import volume and value have \n",
      "increased. Most of the imported fish products are fish and marine mammal fats and oils, as \n",
      "well as fresh or chilled whole fish. Most imports are sourced from EU Member States, in \n",
      "particular the UK and the USA.\n",
      "\n",
      "Exports\n",
      "parent_id: aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGU0LnBkZg2\n",
      "chunk_id: b748f0d7f7e0_aHR0cHM6Ly9zdG9yYWdlYWNjb3VudHZsYWR5ODE0MS5ibG9iLmNvcmUud2luZG93cy5uZXQvZGF0YS9maXNoaW5nZ3VpZGU0LnBkZg2_pages_6\n",
      "Score: 0.01666666753590107\n",
      "Content: from the boundary area between Russia and Norway; \n",
      "the Barents Sea is fished under annual agreements with Russia for cod, haddock and \n",
      "capelin.\n",
      "\n",
      "Marine mammals\n",
      "\n",
      "Between three and five commercial vessels hunt seals in the East Ice. Seals may also be \n",
      "hunted for recreational and specific research purposes. There are mainly two species of \n",
      "seal caught, harp and hooded seals. The number of seals caught decreased by a third \n",
      "between 1995 (15,981) and 2002 (10,691). The seal meat is used for human \n",
      "consumption, as are the oils. The fur and the skin is processed into suede and leather for a \n",
      "variety of products.\n",
      "\n",
      "Whaling by Norwegian vessels is carried out in the Norwegian zone of the North Sea, along \n",
      "the coast of northern Norway, eastwards and off Spitzbergen and Jan Mayen. The catch is \n",
      "used for meat, blubber and animal feed. Norwegian minke whales are hunted using \n",
      "ordinary small fishing vessels, approximately 18 metres long, which are licensed for \n",
      "whaling. The vessels are re-rigged for the whaling season and equipped with modern \n",
      "explosive harpoons.\n",
      "\n",
      "\n",
      "\n",
      "Commercial whaling ceased between 1988 and 1992 after the 1986-moratorium imposed \n",
      "by the International Whaling Commission on commercial whaling. However, whaling for \n",
      "research purposes was continued until 1991, when whaling completely ceased. As Norway \n",
      "opposed the decision and the moratorium was not binding, commercial whaling was \n",
      "resumed in 1993. Catch has since increased to 671 in 2002. The number of vessels \n",
      "whaling remained relatively stable between 1998 and 2002 (34).\n",
      "\n",
      "Sami fishery\n",
      "\n",
      "The Sami are indigenous people of Norway. They live throughout the polar regions of \n",
      "Norway, Sweden, Finland and Russia’s Kola peninsula, with the largest population in \n",
      "Norway. They are traditionally nomadic and live off fishing in the fjords in combination with \n",
      "other trades, such as reindeer herding. In Norway the largest concentration of Sami people\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Search\n",
    "query = \"What is the population of Norway?\"  \n",
    "  \n",
    "search_client = SearchClient(AZURE_AI_SEARCH_ENDPOINT, SEARCH_TARGET_INDEX_NAME, credential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY))\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"vector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    top=2\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
