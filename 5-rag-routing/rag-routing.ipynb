{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Routing\n",
    "\n",
    "In this notebook there'll be covered the following **routing options**:\n",
    "\n",
    "1. **Completion Routers** - LLM Completion Routers use an LLM completion call to return a single word that best describes the query from a list of word options provided in the prompt. This word is then used as part of an If/Else condition to control the application's flow.\n",
    "\n",
    "2. **Function Calling Routers** - LLM Function Calling Routers leverage the function-calling ability of LLMs to pick a route to traverse. Routes are set up as functions with appropriate descriptions, and based on the query, the LLM returns the correct function to use.\n",
    "\n",
    "3. **Semantic Routers** - Semantic Routers use embeddings and similarity searches to select the best route. Each route has associated example queries that are embedded and stored as vectors; the incoming query is embedded, and a similarity search determines the closest match.\n",
    "\n",
    "4. **Zero Shot Classification Routers** - Zero Shot Classification Routers use a Zero-Shot Classification model to assign a label to a piece of text from a predefined set of labels. They can classify new examples from previously unseen classes, making them versatile for various queries.\n",
    "\n",
    "5. **Language Classification Routers** - Language Classification Routers identify the language of the query and route it accordingly. They are useful for applications requiring multilingual parsing capabilities.\n",
    "\n",
    "6. **Keyword Routers** - Keyword Routers select a route by matching keywords between the query and predefined route lists. They can be powered by LLMs or other keyword matching libraries.\n",
    "\n",
    "7. **Logical Routers** - Logical Routers use logic checks against variables such as string lengths, file names, and value comparisons to handle query routing. They rely on existing and discrete variables rather than natural language understanding.\n",
    "\n",
    "One day maybe I'll add some pretty graphics here ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Completion Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design prompt\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "\n",
    "    You are a brilliant assistant who's exceptional in classification tasks.\n",
    "    Your main task is to classify user's query below as either being about `Coffee`, `Tee`, `Soft Drinks`, `Alcoholic Drinks` or `Other`.\n",
    "\n",
    "    Do not respond with more than one word.\n",
    "\n",
    "    <user query>\n",
    "    {user_query}\n",
    "    </user query>\n",
    "\n",
    "    Classification:\n",
    "    \"\"\",\n",
    "    input_variables=[\"user_query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Where can I find kenyan K7 or Ruiru 11 sorts?\" # K7 and Ruiru 11 are popular kenyan coffee sorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTE: Coffee\n"
     ]
    }
   ],
   "source": [
    "# complete router\n",
    "\n",
    "llama = ChatOllama(model=\"llama3\", temperature=0)\n",
    "\n",
    "completion_route_chain = prompt | llama | StrOutputParser()\n",
    "\n",
    "input_data = {\n",
    "    \"user_query\": user_query\n",
    "}\n",
    "\n",
    "route = completion_route_chain.invoke(input=input_data)\n",
    "print(f\"ROUTE: {route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Calling Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be added soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BEST ROUTE: hunting\n"
     ]
    }
   ],
   "source": [
    "# use either semantic_router library or create a custom Route class from the one below\n",
    "\n",
    "emb_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "class Route:\n",
    "    def __init__(self, name, utterances, embedding_model_name=emb_model):\n",
    "        self.name = name\n",
    "        self.utterances = utterances\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        self.model = AutoModel.from_pretrained(embedding_model_name)\n",
    "        self.embeddings = self._embed_utterances(utterances)\n",
    "\n",
    "    def _embed_utterances(self, utterances):\n",
    "        # tokenize utterances\n",
    "        tokens = self.tokenizer(utterances, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        # get embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "        return embeddings\n",
    "\n",
    "def embed_query(query, embedding_model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    model = AutoModel.from_pretrained(embedding_model_name)\n",
    "    tokens = tokenizer(query, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embedding = model(**tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "    return embedding\n",
    "\n",
    "def find_best_route(query, routes):\n",
    "    query_embedding = embed_query(query)\n",
    "    best_match_route = None\n",
    "    highest_similarity = -1\n",
    "    \n",
    "    for route in routes:\n",
    "        similarities = cosine_similarity(query_embedding, route.embeddings).flatten()\n",
    "        max_similarity = np.max(similarities)\n",
    "        \n",
    "        if max_similarity > highest_similarity:\n",
    "            highest_similarity = max_similarity\n",
    "            best_match_route = route\n",
    "            \n",
    "    return best_match_route\n",
    "\n",
    "# example routing\n",
    "fishing = Route(\n",
    "    name=\"fishing\",\n",
    "    utterances=[\n",
    "        \"What's the best bait for catching bass?\",\n",
    "        \"Do you prefer freshwater or saltwater fishing?\",\n",
    "        \"What's your favorite fishing spot?\",\n",
    "        \"Have you ever caught a really big fish?\",\n",
    "        \"Any tips for a beginner fisherman?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "hunting = Route(\n",
    "    name=\"hunting\",\n",
    "    utterances=[\n",
    "        \"What's the best time of year for deer hunting?\",\n",
    "        \"Do you use a bow or a rifle?\",\n",
    "        \"What's your most memorable hunting trip?\",\n",
    "        \"How do you track game in the wild?\",\n",
    "        \"Any tips for staying safe while hunting?\",\n",
    "        \"Ducks hunting tips\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "camping = Route(\n",
    "    name=\"camping\",\n",
    "    utterances=[\n",
    "        \"What's your favorite camping spot?\",\n",
    "        \"Do you prefer tents or RVs for camping?\",\n",
    "        \"How do you make a campfire?\",\n",
    "        \"What's your go-to camping meal?\",\n",
    "        \"Any tips for a first-time camper?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "routes = [fishing, hunting, camping]\n",
    "\n",
    "query = \"I am looking for a sea near-shore location for hunting ducks\"\n",
    "best_route = find_best_route(query, routes)\n",
    "print(f\"THE BEST ROUTE: {best_route.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zero Shot Classification Router\n",
    "\n",
    "The implementation can be found on the Haystack GitHub [here](https://github.com/deepset-ai/haystack/blob/main/haystack/components/routers/zero_shot_text_router.py#L130) ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Language Classification Router\n",
    "\n",
    "Practically, there are two options how to establish routing based on multiple languages.\n",
    "\n",
    "- **Option 1**: Utilize external services for language detection (e.g. Azure Speech)\n",
    "- **Option 2**: Do the translation and routing via Prompt Engineering (example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design prompt\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "\n",
    "    You are a brilliant assistant who's exceptional in language identification tasks.\n",
    "    Your main task is to identify the language of the user's query below and respond using one of the ISO 639 langauge codes.\n",
    "\n",
    "    Do not respond with more than one word.\n",
    "\n",
    "    <ISO codes>\n",
    "    {iso_codes}\n",
    "    </ISO codes>\n",
    "\n",
    "    <user query>\n",
    "    {user_query}\n",
    "    </user query>\n",
    "\n",
    "    Language:\n",
    "    \"\"\",\n",
    "    input_variables=[\"iso_codes\", \"user_query\"],\n",
    ")\n",
    "\n",
    "iso_639_languages = {\n",
    "    \"English\": \"en\",\n",
    "    \"Mandarin Chinese\": \"zh\",\n",
    "    \"Hindi\": \"hi\",\n",
    "    \"Spanish\": \"es\",\n",
    "    \"French\": \"fr\",\n",
    "    \"German\": \"de\",\n",
    "    \"Standard Arabic\": \"ar\",\n",
    "    \"Bengali\": \"bn\",\n",
    "    \"Portuguese\": \"pt\",\n",
    "    \"Russian\": \"ru\",\n",
    "    \"Japanese\": \"ja\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "\n",
    "query = \"Was macht man am Freitag Abend in Berlin?\" # german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTE: de\n"
     ]
    }
   ],
   "source": [
    "# language router\n",
    "\n",
    "llama = ChatOllama(model=\"llama3\", temperature=0)\n",
    "\n",
    "completion_route_chain = prompt | llama | StrOutputParser()\n",
    "\n",
    "input_data = {\n",
    "    \"iso_codes\": iso_639_languages,\n",
    "    \"user_query\": query   \n",
    "}\n",
    "\n",
    "route = completion_route_chain.invoke(input=input_data)\n",
    "print(f\"ROUTE: {route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Keyword Router\n",
    "\n",
    "A keyword router will select a route by matching **keywords** between the **user's query** and **routes list**. In some specific use cases, we only need a couple of keywords to route the query to a specific module or handler. \n",
    "\n",
    "Why do we need to make extra LLM calls, if we can save some **latency** and **extra money**?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTE: web\n"
     ]
    }
   ],
   "source": [
    "# OPTION 1: simple keyword router\n",
    "\n",
    "class KeywordRouter:\n",
    "    def __init__(self, routes):\n",
    "        self.routes = routes\n",
    "\n",
    "    def find_keyword_route(self, query):\n",
    "        query_lower = query.lower()\n",
    "        for route, keywords in self.routes.items():\n",
    "            if any(keyword in query_lower for keyword in keywords):\n",
    "                return route\n",
    "        return \"default\"\n",
    "\n",
    "# define routes --> better descriptions = better routing\n",
    "routes = {\n",
    "    \"web\": [\"html\", \"css\", \"javascript\", \"web\", \"website\", \"frontend\", \"backend\"],\n",
    "    \"blockchain\": [\"blockchain\", \"crypto\", \"bitcoin\", \"ethereum\", \"smart contract\", \"decentralized\"],\n",
    "    \"opensource\": [\"open-source\", \"open source\", \"github\", \"git\", \"contribution\", \"license\"],\n",
    "}\n",
    "\n",
    "user_query = \"How to be a frontend developer?\"\n",
    "\n",
    "keyword_router = KeywordRouter(routes=routes)\n",
    "\n",
    "route = keyword_router.find_keyword_route(query=user_query)\n",
    "print(f\"ROUTE: {route}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: keyword router w/ retrieval --> 1st step is to create retrievers (simulation of multiple routes --> web/blockchain/opensource)\n",
    "\n",
    "emb_model = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL CHUNKS FOR WEB: 49\n",
      "TOTAL CHUNKS FOR OPENSOURCE: 15\n",
      "TOTAL CHUNKS FOR BLOCKCHAIN: 35\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "\n",
    "data_folder = \"../data/rag-routing\"\n",
    "documents = {\n",
    "    \"blockchain\": os.path.join(data_folder, \"blockchain.pdf\"),\n",
    "    \"opensource\": os.path.join(data_folder, \"opensource.pdf\"),\n",
    "    \"web\": os.path.join(data_folder, \"web.pdf\")\n",
    "}\n",
    "\n",
    "loaders = {name: PyPDFLoader(path) for name, path in documents.items()}\n",
    "docs_content = {name: loader.load() for name, loader in loaders.items()}\n",
    "text_splitter = NLTKTextSplitter()\n",
    "chunked_docs = {name: text_splitter.split_documents(content) for name, content in docs_content.items()}\n",
    "print(f\"TOTAL CHUNKS FOR WEB: {len(chunked_docs.get('web'))}\")\n",
    "print(f\"TOTAL CHUNKS FOR OPENSOURCE: {len(chunked_docs.get('opensource'))}\")\n",
    "print(f\"TOTAL CHUNKS FOR BLOCKCHAIN: {len(chunked_docs.get('blockchain'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector stores for each doc --> simulation of different pipelines/handlers/search indexes, etc.\n",
    "\n",
    "emb_model = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-large\")\n",
    "vector_stores = {}\n",
    "for name, chunks in chunked_docs.items():\n",
    "    db = Chroma.from_documents(documents=chunks, embedding=emb_model)\n",
    "    vector_stores[name] = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retrievers\n",
    "\n",
    "retrievers = {name: chroma_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3}) for name, chroma_db in vector_stores.items()}\n",
    "\n",
    "web_retriever = retrievers.get('web') # route 1\n",
    "blockchain_retriever = retrievers.get('blockchain') # route 2\n",
    "opensource_retriever = retrievers.get('opensource') # route 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
