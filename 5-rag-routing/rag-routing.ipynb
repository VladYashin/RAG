{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Routing\n",
    "\n",
    "In this notebook there'll be covered the following **routing options**:\n",
    "\n",
    "1. **Completion Routers** - LLM Completion Routers use an LLM completion call to return a single word that best describes the query from a list of word options provided in the prompt. This word is then used as part of an If/Else condition to control the application's flow.\n",
    "\n",
    "2. **Function Calling Routers** - LLM Function Calling Routers leverage the function-calling ability of LLMs to pick a route to traverse. Routes are set up as functions with appropriate descriptions, and based on the query, the LLM returns the correct function to use.\n",
    "\n",
    "3. **Semantic Routers** - Semantic Routers use embeddings and similarity searches to select the best route. Each route has associated example queries that are embedded and stored as vectors; the incoming query is embedded, and a similarity search determines the closest match.\n",
    "\n",
    "4. **Zero Shot Classification Routers** - Zero Shot Classification Routers use a Zero-Shot Classification model to assign a label to a piece of text from a predefined set of labels. They can classify new examples from previously unseen classes, making them versatile for various queries.\n",
    "\n",
    "5. **Language Classification Routers** - Language Classification Routers identify the language of the query and route it accordingly. They are useful for applications requiring multilingual parsing capabilities.\n",
    "\n",
    "6. **Keyword Routers** - Keyword Routers select a route by matching keywords between the query and predefined route lists. They can be powered by LLMs or other keyword matching libraries.\n",
    "\n",
    "7. **Logical Routers** - Logical Routers use logic checks against variables such as string lengths, file names, and value comparisons to handle query routing. They rely on existing and discrete variables rather than natural language understanding.\n",
    "\n",
    "One day maybe I'll add some pretty graphics here ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set\n",
    "\n",
    "# DS & ML\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# LangChain\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# NLTK downloads if needed (uncomment the following lines to download)\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Azure AI Search\n",
    "AZURE_AI_SEARCH_API_KEY = os.environ.get('AZURE_AI_SEARCH_API_KEY')\n",
    "AZURE_AI_SEARCH_ENDPOINT = os.environ.get('AZURE_AI_SEARCH_ENDPOINT')\n",
    "AZURE_AI_ROUTING_INDEX = os.environ.get('AZURE_AI_ROUTING_INDEX')\n",
    "\n",
    "# Azure OpenAI\n",
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_VERSION = os.environ.get('AZURE_OPENAI_VERSION')\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.environ.get('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Completion Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design prompt\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "\n",
    "    You are a brilliant assistant who's exceptional in classification tasks.\n",
    "    Your main task is to classify user's query below as either being about `Coffee`, `Tee`, `Soft Drinks`, `Alcoholic Drinks` or `Other`.\n",
    "\n",
    "    Do not respond with more than one word.\n",
    "\n",
    "    <user query>\n",
    "    {user_query}\n",
    "    </user query>\n",
    "\n",
    "    Classification:\n",
    "    \"\"\",\n",
    "    input_variables=[\"user_query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Where can I find kenyan K7 or Ruiru 11 sorts?\" # K7 and Ruiru 11 are popular kenyan coffee sorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTE: Coffee\n"
     ]
    }
   ],
   "source": [
    "# complete router\n",
    "\n",
    "llama = ChatOllama(model=\"llama3\", temperature=0)\n",
    "\n",
    "completion_route_chain = prompt | llama | StrOutputParser()\n",
    "\n",
    "input_data = {\n",
    "    \"user_query\": user_query\n",
    "}\n",
    "\n",
    "route = completion_route_chain.invoke(input=input_data)\n",
    "print(f\"ROUTE: {route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Calling Router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation can be found on the LlamaIndex [here](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/) ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BEST ROUTE: hunting\n"
     ]
    }
   ],
   "source": [
    "# use either semantic_router library or create a custom Route class from the one below\n",
    "\n",
    "emb_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "class Route:\n",
    "    def __init__(self, name, utterances, embedding_model_name=emb_model):\n",
    "        self.name = name\n",
    "        self.utterances = utterances\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        self.model = AutoModel.from_pretrained(embedding_model_name)\n",
    "        self.embeddings = self._embed_utterances(utterances)\n",
    "\n",
    "    def _embed_utterances(self, utterances):\n",
    "        # tokenize utterances\n",
    "        tokens = self.tokenizer(utterances, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        # get embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "        return embeddings\n",
    "\n",
    "def embed_query(query, embedding_model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    model = AutoModel.from_pretrained(embedding_model_name)\n",
    "    tokens = tokenizer(query, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embedding = model(**tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "    return embedding\n",
    "\n",
    "def find_best_route(query, routes):\n",
    "    query_embedding = embed_query(query)\n",
    "    best_match_route = None\n",
    "    highest_similarity = -1\n",
    "    \n",
    "    for route in routes:\n",
    "        similarities = cosine_similarity(query_embedding, route.embeddings).flatten()\n",
    "        max_similarity = np.max(similarities)\n",
    "        \n",
    "        if max_similarity > highest_similarity:\n",
    "            highest_similarity = max_similarity\n",
    "            best_match_route = route\n",
    "            \n",
    "    return best_match_route\n",
    "\n",
    "# example routing\n",
    "fishing = Route(\n",
    "    name=\"fishing\",\n",
    "    utterances=[\n",
    "        \"What's the best bait for catching bass?\",\n",
    "        \"Do you prefer freshwater or saltwater fishing?\",\n",
    "        \"What's your favorite fishing spot?\",\n",
    "        \"Have you ever caught a really big fish?\",\n",
    "        \"Any tips for a beginner fisherman?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "hunting = Route(\n",
    "    name=\"hunting\",\n",
    "    utterances=[\n",
    "        \"What's the best time of year for deer hunting?\",\n",
    "        \"Do you use a bow or a rifle?\",\n",
    "        \"What's your most memorable hunting trip?\",\n",
    "        \"How do you track game in the wild?\",\n",
    "        \"Any tips for staying safe while hunting?\",\n",
    "        \"Ducks hunting tips\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "camping = Route(\n",
    "    name=\"camping\",\n",
    "    utterances=[\n",
    "        \"What's your favorite camping spot?\",\n",
    "        \"Do you prefer tents or RVs for camping?\",\n",
    "        \"How do you make a campfire?\",\n",
    "        \"What's your go-to camping meal?\",\n",
    "        \"Any tips for a first-time camper?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "routes = [fishing, hunting, camping]\n",
    "\n",
    "query = \"I am looking for a sea near-shore location for hunting ducks\"\n",
    "best_route = find_best_route(query, routes)\n",
    "print(f\"THE BEST ROUTE: {best_route.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zero Shot Classification Router\n",
    "\n",
    "The implementation can be found on the Haystack GitHub [here](https://github.com/deepset-ai/haystack/blob/main/haystack/components/routers/zero_shot_text_router.py#L130) ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Language Classification Router\n",
    "\n",
    "Practically, there are two options how to establish routing based on multiple languages.\n",
    "\n",
    "- **Option 1**: Utilize external services for language detection (e.g. Azure Speech)\n",
    "- **Option 2**: Do the translation and routing via Prompt Engineering (example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design prompt\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "\n",
    "    You are a brilliant assistant who's exceptional in language identification tasks.\n",
    "    Your main task is to identify the language of the user's query below and respond using one of the ISO 639 langauge codes.\n",
    "\n",
    "    Do not respond with more than one word.\n",
    "\n",
    "    <ISO codes>\n",
    "    {iso_codes}\n",
    "    </ISO codes>\n",
    "\n",
    "    <user query>\n",
    "    {user_query}\n",
    "    </user query>\n",
    "\n",
    "    Language:\n",
    "    \"\"\",\n",
    "    input_variables=[\"iso_codes\", \"user_query\"],\n",
    ")\n",
    "\n",
    "iso_639_languages = {\n",
    "    \"English\": \"en\",\n",
    "    \"Mandarin Chinese\": \"zh\",\n",
    "    \"Hindi\": \"hi\",\n",
    "    \"Spanish\": \"es\",\n",
    "    \"French\": \"fr\",\n",
    "    \"German\": \"de\",\n",
    "    \"Standard Arabic\": \"ar\",\n",
    "    \"Bengali\": \"bn\",\n",
    "    \"Portuguese\": \"pt\",\n",
    "    \"Russian\": \"ru\",\n",
    "    \"Japanese\": \"ja\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "\n",
    "query = \"Was macht man am Freitag Abend in Berlin?\" # german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTE: de\n"
     ]
    }
   ],
   "source": [
    "# language router\n",
    "\n",
    "llama = ChatOllama(model=\"llama3\", temperature=0)\n",
    "\n",
    "completion_route_chain = prompt | llama | StrOutputParser()\n",
    "\n",
    "input_data = {\n",
    "    \"iso_codes\": iso_639_languages,\n",
    "    \"user_query\": query   \n",
    "}\n",
    "\n",
    "route = completion_route_chain.invoke(input=input_data)\n",
    "print(f\"ROUTE: {route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Keyword Router\n",
    "\n",
    "A keyword router will select a route by matching **keywords** between the **user's query** and **routes list**. In some specific use cases, we only need a couple of keywords to route the query to a specific module or handler. \n",
    "\n",
    "Why do we need to make extra LLM calls, if we can save some **latency** and **extra money**?!\n",
    "\n",
    "#### OPTION 1: Simple Keyword Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTE: web\n"
     ]
    }
   ],
   "source": [
    "# OPTION 1: simple keyword router\n",
    "\n",
    "class KeywordRouter:\n",
    "    def __init__(self, routes):\n",
    "        self.routes = routes\n",
    "\n",
    "    def find_keyword_route(self, query):\n",
    "        query_lower = query.lower()\n",
    "        for route, keywords in self.routes.items():\n",
    "            if any(keyword in query_lower for keyword in keywords):\n",
    "                return route\n",
    "        return \"default\"\n",
    "\n",
    "# define routes --> better descriptions = better routing\n",
    "routes = {\n",
    "    \"web\": [\"html\", \"css\", \"javascript\", \"web\", \"website\", \"frontend\", \"backend\"],\n",
    "    \"blockchain\": [\"blockchain\", \"crypto\", \"bitcoin\", \"ethereum\", \"smart contract\", \"decentralized\"],\n",
    "    \"opensource\": [\"open-source\", \"open source\", \"github\", \"git\", \"contribution\", \"license\"],\n",
    "}\n",
    "\n",
    "user_query = \"How to be a frontend developer?\"\n",
    "\n",
    "keyword_router = KeywordRouter(routes=routes)\n",
    "\n",
    "route = keyword_router.find_keyword_route(query=user_query)\n",
    "print(f\"ROUTE: {route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTION 2: Keyword Router w/ Retrieval\n",
    "\n",
    "In this step we'll create **two different retrievers** to simulate **two different pipelines**. \n",
    "\n",
    "- **Retriever 1**: Documents about Web, Open Source & Blockchain will be stored in the Chroma DB\n",
    "- **Retriever 2**: Documents about fishing will be stored in the Azure Vector Store\n",
    "\n",
    "The main goal is to select the best possible route by analyzing the amount & relevance of given keywords in top n extracted documents and return the retriever with the documents of higher relevance to the user's query. \n",
    "\n",
    "âš¡ **Flow**: user_query --> retrieval --> keyword relevance calculation --> return either **Retriever 1** or **Retriever 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NO. OF CHUNKS FOR 1st RETRIEVER:  99\n"
     ]
    }
   ],
   "source": [
    "# DOCUMENT PROCESSING FOR RETRIEVER 1\n",
    "emb_model = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-large\")\n",
    "\n",
    "data_path = \"../data/rag-routing\"\n",
    "pdf_files = [f for f in os.listdir(data_path) if f.endswith('.pdf')]\n",
    "data = [PyPDFLoader(os.path.join(data_path, file)).load() for file in pdf_files]\n",
    "docs_list = [item for sublist in data for item in sublist]\n",
    "text_splitter = NLTKTextSplitter()\n",
    "doc_chunks = text_splitter.split_documents(docs_list)\n",
    "\n",
    "print(\"TOTAL NO. OF CHUNKS FOR 1st RETRIEVER: \", len(doc_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st retriever representing the first route (Chroma DB)\n",
    "\n",
    "chroma_db = Chroma.from_documents(documents=doc_chunks, embedding=emb_model)\n",
    "retriever = chroma_db.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NO. OF CHUNKS FOR 2nd RETRIEVER:  66\n"
     ]
    }
   ],
   "source": [
    "# DOCUMENT PROCESSING FOR RETRIEVER 2\n",
    "\n",
    "data_path = \"../data\"\n",
    "files = [\"fishingguide1.pdf\", \"fishingguide2.pdf\"]\n",
    "\n",
    "azure_data = [PyPDFLoader(os.path.join(data_path, file)).load() for file in files]\n",
    "azure_docs_list = [item for sublist in azure_data for item in sublist]\n",
    "text_splitter = NLTKTextSplitter()\n",
    "azure_doc_chunks = text_splitter.split_documents(azure_docs_list)\n",
    "\n",
    "print(\"TOTAL NO. OF CHUNKS FOR 2nd RETRIEVER: \", len(azure_doc_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZTZmMTAzZDAtOWZmZC00ZGM1LThmMGMtNDUyM2MwN2VlNDgz',\n",
       " 'YTUzZmM5ZDctZDFkZS00MDhjLTkyYmItNTBmZjVmNjRlYTFm',\n",
       " 'ZDMyZDFmOWYtMWRlNy00ZTgyLWJjMmYtZjczM2E2MzAwYWU1',\n",
       " 'MGYyYjJhODktYWU3NS00YWJmLWE4MzMtMTM2MDVmNzY0MTRh',\n",
       " 'ZWQzZWZhNWUtN2E2MS00Zjg0LTg0M2ItZDVkMWUzMDllYzI2',\n",
       " 'ZmM1NzIzNTAtOTM2YS00YWJkLTkzMmMtNTYzODJkNjA3N2Iz',\n",
       " 'N2E5ZjUxYmUtNzY0OS00NDQ3LTg1YzUtZjlmM2Y2NWIyMTVl',\n",
       " 'N2I3ZjgxZDgtZThjYS00ZDE5LWFjN2MtZDQ2N2RkYjE3ZDAy',\n",
       " 'M2Q4YzEyNDQtYzAzYi00YmFjLThmNGUtZDMwNTI0OTYyY2Vi',\n",
       " 'NTgxMTlkYzktZjhiNi00MTU5LThlMDItNWQ5MWQ1NTA1NzNi',\n",
       " 'YjZmMzY2MjktODYxNi00YmQ2LThjYjYtZmUyNzYzNWQyMGM5',\n",
       " 'NWJjNDk0MjAtMjllZS00YzcxLTgyNWMtM2FmOWU5ODFmNGI3',\n",
       " 'NDY2ZTU2YTAtYTUwMi00YjRjLThiODUtOWI2N2VjM2U3YjNh',\n",
       " 'MmYwNjljZjYtMjBlMy00M2JhLTk3MzktNDcyNzRmOTdhYzE0',\n",
       " 'NzUxMmMyMDktNGU1ZC00YzhhLThmNDktOTNjOGRlNTIyOThh',\n",
       " 'ZWFmYjU4ZWMtZDg5Mi00NWM1LTgxOGItMDk0NTY2YTQxOTA1',\n",
       " 'YTlmYThhMmMtMDgyNy00ODg4LTgyZmYtNTI3Yjk2NTdiNmJh',\n",
       " 'NjYzMjhlOTAtY2VkMy00MzcwLTgxYjktODE0NWEzYWIyNTI0',\n",
       " 'NWIwMjE1OTUtY2EzOS00NDFiLTk5OGEtNjI5MjkyM2YwOWQy',\n",
       " 'NzA1MTE5ZTQtZGY1Ni00MGJlLThmOTUtNGY1ZDM4NWQ4NmIw',\n",
       " 'NzFmYWFkM2ItYmI2Zi00MTYyLWI4ZTQtM2JhZWU3MzAyY2Q0',\n",
       " 'MDZmMzZiZDUtMWY5OS00ZGQ4LTkxMzEtMzgxZDliYWI4MzUw',\n",
       " 'MDUyODZhZGUtNzA1ZC00NWJkLWJiNWYtZjAxNDk3ZTQzZTQw',\n",
       " 'NWU3Njc2NTEtOTc1Yy00Y2RmLTk2NzMtMjNhOWI0ZjgwNjZl',\n",
       " 'OGY4MGM3MDAtMDcwZC00ZmVmLWFiMDUtNDBiNDk3NDlhOTM5',\n",
       " 'ZWUxZDM5MGMtOThkMC00Nzc5LWE4NTctZTZjY2FmYjNjNmM1',\n",
       " 'ODdiZjUwZDctNTU0OC00YjEyLWIxNmEtYWE4MDEzZjhhNGI5',\n",
       " 'ZDVkNjZhNjktNDA1Yy00MDI2LTkxZGYtMDhmNjAxODdjZGZh',\n",
       " 'NmZlZTk2MjEtMWI1YS00ZGY5LWFhNDAtZWQ2ZDZkNmM3NzY4',\n",
       " 'OTQ4NjQ1Y2ItYmViMi00YTgzLWE0NmQtYWNiYjZhNjc5YzJi',\n",
       " 'NGE4Y2NlZjYtMzUxOS00NDcxLTg0ZmItNDllYjA0Y2Q0M2Jm',\n",
       " 'N2RmNTQ3ZGQtZmFiNS00ODE4LTk4NDEtMDZkZWNjZTViZjg3',\n",
       " 'MGNmMDUyOWMtOGE2Yy00NDk2LTlmMjUtNDc0ZGJmNmE5MTAz',\n",
       " 'MzI5OWE0ODAtMTRhOC00M2YyLThiNTktNzI5MDkzZmE4MTYz',\n",
       " 'OTIwNDRjZGItMzFjMy00NzczLTlmOTAtODA5NjZjNmM3MWNl',\n",
       " 'Y2QwMGM1OWItMWRhMy00ZTZkLTk4MDUtN2Q2YzJjOTUyN2Zh',\n",
       " 'MjcyZTFjZDktMGVkYy00ZDE3LTg0NWMtODBmNzk5NjczZmRj',\n",
       " 'OWYzYmY3ODUtYmM5My00MjIxLWE1OGYtNzcwNmZiYzIzYWZh',\n",
       " 'NzNiY2QxZWUtODBkNy00ZmUzLWI4MjItNWEwY2MwMjgyOWQ4',\n",
       " 'MGMyYTliZTItNzc1Mi00YmU3LWI0Y2UtYmUxZDFlY2UwNjM0',\n",
       " 'YWQ3NmFhMmUtMGQyNS00ODdiLWFlYzMtODAyZDQ0NmQwYzRh',\n",
       " 'Zjk5ZmQ1NTMtYThjYS00ZWM5LWFlOWQtMjhmZmNkMTgxZDFk',\n",
       " 'ZmExZWVlMGQtZTg0Yi00NzkxLThiOGItMTJmMTE3MjY3OTBh',\n",
       " 'Zjc4NWE3MGEtMzQ3Mi00ZDI3LThhNDMtYTkwMzBjOGI4NjY4',\n",
       " 'ZjZjODE3M2QtMTdmMi00ZWQyLTg0ZDUtODgyNzY3MGFkMTE5',\n",
       " 'ZDMyNmU3NjYtYjNmNy00MDFlLTkxZTAtMGJjMWE2ZDhiMTBj',\n",
       " 'NzljMzE3ODYtOTVlNC00MGNkLTg5MzktM2E5ODAyNWU3NDMz',\n",
       " 'YWMzOTU1YWEtYTMzOS00ZjczLTliODItYjZjOTA5M2UxZDYw',\n",
       " 'NTQzNTc1NjctODk5NS00NDA2LTk1YTUtMTI3ODBjNGYyMjBh',\n",
       " 'ZWYyYjhhYTktY2Q0YS00M2RkLTg1ZDktMWZjNmE4ZTBlYWY3',\n",
       " 'Zjc4OThmYzMtOTRmMS00NzIzLWJmODItMGM3N2EzYWZkOWNj',\n",
       " 'ZWM0N2M3YWQtMDEyZS00YmExLTllMTYtZTI2ZWViOGVlZmYx',\n",
       " 'ZWE0M2EyOTYtZDdmYy00Y2Q3LWFmZmItMGQ5MTE4MTFiMWFj',\n",
       " 'MzcwYzMzMzctM2IxNC00MTdiLTg5MzktZWVmNTc1OGRkMjJi',\n",
       " 'MDRmMDlkYWUtN2UyMi00ZjllLWIzNWUtYWQwNTM4NWVkZDBl',\n",
       " 'MTc2YjZjY2EtNGQ2Zi00MDNkLTg2ZDUtZjJlNzBmZjczODhl',\n",
       " 'MDY2OTlhMTUtZmNkYy00ODBhLTgzOTMtZDM3ZGI4MGMwNWRm',\n",
       " 'NmE4ODdhOWItMDczZi00MTU3LTg1ZmEtMmI4MDU1Y2IwMDEx',\n",
       " 'NDRiYTNhZjUtNDFiYi00Y2VlLWEyNDYtY2I1YTYzY2MwM2I0',\n",
       " 'NWZjZTBmZjItYThhNS00MThhLWJhOGEtOTBkMDJhZGVjZTEw',\n",
       " 'ZTgyMDQ5NjAtNTFmNi00YThmLWFjNGMtNjNjMTUwZGM1OWM0',\n",
       " 'NmU5YTRhNjctZWE4ZC00ZDBiLTlkODYtNTdmNWVlMWYxMTcx',\n",
       " 'ODI0NWM1NTItNDZiMi00NjEzLWIzMTItYzg1NWNmZTc1YTFl',\n",
       " 'MGFiODhhMzQtYjJkMy00MWFkLTljNDktNTFiNDYxMzgxZGI5',\n",
       " 'MDYzNGU3MDgtNWZlYy00ZWMwLTliYjMtMTQ1MGYxOWU1MjUy',\n",
       " 'ZmNkNDI1NWYtMDAyNy00NjM2LWE5MmMtZDdhZDhhZDE2YmI4']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup vector store\n",
    "\n",
    "azure_oai_emb_model: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    openai_api_version=AZURE_OPENAI_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=AZURE_AI_SEARCH_ENDPOINT,\n",
    "    azure_search_key=AZURE_AI_SEARCH_API_KEY,\n",
    "    index_name=AZURE_AI_ROUTING_INDEX,\n",
    "    embedding_function=azure_oai_emb_model.embed_query,\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=azure_doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd retriever representing the second route (Azure Vector Store)\n",
    "\n",
    "azure_retriever = vector_store.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the keyword relevance score based on the extracted documents using string preprocessing & Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess(text: str) -> Set[str]:\n",
    "    words = word_tokenize(text.lower())\n",
    "    return {ps.stem(word) for word in words if word.isalnum() and word not in stop_words}\n",
    "\n",
    "# jaccard similarity\n",
    "def jaccard_similarity(query_set: Set[str], doc: str) -> float:\n",
    "    doc_set = preprocess(doc)\n",
    "    intersection = len(query_set & doc_set)\n",
    "    union = len(query_set | doc_set)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# compute similarities\n",
    "def compute_similarities(preprocessed_query: Set[str], documents: List[Any]) -> List[float]:\n",
    "    return [jaccard_similarity(preprocessed_query, doc.page_content) for doc in documents]\n",
    "\n",
    "# routing\n",
    "def jaccard_keyword_routing(retrievers: Dict[str, Any], query: str) -> Dict[str, float]:\n",
    "    scores = {}\n",
    "    preprocessed_query = preprocess(query)\n",
    "\n",
    "    for key, retriever in retrievers.items():\n",
    "        docs = retriever.invoke(input=query)\n",
    "        if docs:\n",
    "            similarity_scores = compute_similarities(preprocessed_query, docs)\n",
    "            avg_score = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0\n",
    "        else:\n",
    "            avg_score = 0\n",
    "        scores[key] = avg_score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chroma_db_route': 0.0, 'azure_vector_store_route': 0.0672093837535014}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "retrievers = {\"chroma_db_route\": retriever,\n",
    "              \"azure_vector_store_route\": azure_retriever}\n",
    "user_query = \"What are the best fishing spots for fishing with family?\"\n",
    "\n",
    "scores = jaccard_keyword_routing(retrievers, user_query)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logical Router\n",
    "\n",
    "A logical router takes conditions that you specify and routes your data through different paths down the pipeline.\n",
    "\n",
    "Example conditions:\n",
    "- Query input length\n",
    "- Number of specified values from the query\n",
    "- Special characters in the query (e.g. ?!%$&)\n",
    "- Number of specific words (e.g. \"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for Query Length: True\n",
      "Response for Specific Values: False\n",
      "Response for Special Characters: True\n",
      "Response for Specific Word Count: True\n"
     ]
    }
   ],
   "source": [
    "# query length\n",
    "\n",
    "def query_length_router(query:str):\n",
    "    if len(query.split()) > 2:\n",
    "        response = retriever.invoke(input=query)\n",
    "        output = {\n",
    "            \"sufficient_length\": True,\n",
    "            \"query\": query,\n",
    "            \"response\": response\n",
    "        }\n",
    "        return output\n",
    "    else:\n",
    "        return {\"sufficient_length\": False} # route to the needed pipeline\n",
    "    \n",
    "\n",
    "# number of specific values from the query\n",
    "def query_value_count_router(query: str):\n",
    "    values = re.findall(r'\\d+', query)  # find all digit sequences\n",
    "    if len(values) >= 3:  # check if there are 3 numbers\n",
    "        response = retriever.invoke(input=query)\n",
    "        return {\n",
    "            \"sufficient_values\": True,\n",
    "            \"query\": query,\n",
    "            \"response\": response\n",
    "        }\n",
    "    else:\n",
    "        return {\"sufficient_values\": False} # route to the needed pipeline\n",
    "\n",
    "# special characters\n",
    "def special_characters_router(query: str):\n",
    "    if any(char in set('?!%$&') for char in query):  # check for special characters\n",
    "        response = retriever.invoke(input=query)\n",
    "        return {\n",
    "            \"contains_special_chars\": True,\n",
    "            \"query\": query,\n",
    "            \"response\": response\n",
    "        }\n",
    "    else:\n",
    "        return {\"contains_special_chars\": False} # route to the needed pipeline\n",
    "\n",
    "\n",
    "# specific words\n",
    "def specific_word_count_router(query: str, word: str = \"bitcoin\"):\n",
    "    count = query.lower().split().count(word.lower())\n",
    "    if count > 0:\n",
    "        response = retriever.invoke(input=query)\n",
    "        return {\n",
    "            \"word_count_sufficient\": True,\n",
    "            \"query\": query,\n",
    "            \"response\": response\n",
    "        }\n",
    "    else:\n",
    "        return {\"word_count_sufficient\": False} # route to the needed pipeline\n",
    "\n",
    "\n",
    "user_query = \"What was the bitcoin price between 2020 and 2024?\"\n",
    "\n",
    "response_query_length = query_length_router(user_query)\n",
    "response_specific_values = query_value_count_router(user_query)\n",
    "response_special_characters = special_characters_router(user_query)\n",
    "response_specific_word_count = specific_word_count_router(user_query, \"bitcoin\")\n",
    "\n",
    "print(\"Response for Query Length:\", response_query_length.get(\"sufficient_length\"))\n",
    "print(\"Response for Specific Values:\", response_specific_values.get(\"sufficient_values\"))\n",
    "print(\"Response for Special Characters:\", response_special_characters.get(\"contains_special_chars\"))\n",
    "print(\"Response for Specific Word Count:\", response_specific_word_count.get(\"word_count_sufficient\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
